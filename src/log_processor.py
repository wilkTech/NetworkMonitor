"""
Procesor logÃ³w Wazuh w czasie rzeczywistym.

Åšledzi plik archives.json (tail -f style) i kategoryzuje kaÅ¼dy wpis.

Å¹rÃ³dÅ‚o danych: plik JSON (archives.json), kaÅ¼da linia lub blok to jeden obiekt JSON
(w formacie Wazuh: timestamp, decoder, rule, full_log, agent, data, location, â€¦).
Walidacja: akceptowane sÄ… tylko obiekty (dict); nie-dict sÄ… pomijane; bÅ‚Ä™dy
kategoryzacji nie odrzucajÄ… wpisu â€“ wpis dostaje _category "unknown".
NiepeÅ‚ny JSON na granicy odczytu jest buforowany do nastÄ™pnego poll().

Wykrywanie rotacji/truncate: gdy plik zostanie obciÄ™ty lub zastÄ…piony (nowy inode),
pozycja jest resetowana, Å¼eby program ponownie zaciÄ…gaÅ‚ dane z aktualnego pliku.
"""

import json
import logging
import os
import time
from collections import deque
from pathlib import Path
from typing import Callable, Optional

from .categorizer import categorize, LogCategory

logger = logging.getLogger(__name__)


class LogProcessor:
    """Przetwarza logi w czasie rzeczywistym i utrzymuje bufor ostatnich wpisÃ³w."""

    # Maks. rozmiar bufora niepeÅ‚nego JSON; przy przekroczeniu bufor jest czyszczony (unikamy zawieszenia na jednym uciÄ™tym obiekcie)
    DEFAULT_MAX_READ_BUFFER_BYTES = 512 * 1024
    # Po tylu kolejnych odczytach bez nowych wpisÃ³w (przy niepustym buforze) wymuszamy resync do koÅ„ca pliku
    STALE_BUFFER_POLLS = 24  # ~12 s przy poll_interval=0.5

    def __init__(
        self,
        archives_path: Path,
        max_logs: int = 5000,
        poll_interval: float = 0.5,
        on_new_entry: Optional[Callable] = None,
        on_read_error: Optional[Callable[[], None]] = None,
        max_read_buffer_bytes: Optional[int] = None,
    ):
        self.archives_path = Path(archives_path)
        self.max_logs = max_logs
        self.poll_interval = poll_interval
        self.on_new_entry = on_new_entry
        self.on_read_error = on_read_error
        self.max_read_buffer_bytes = max_read_buffer_bytes or self.DEFAULT_MAX_READ_BUFFER_BYTES

        self._entries: deque = deque(maxlen=max_logs)
        self._file_position = 0
        self._running = False
        # Bufor na niepeÅ‚ny JSON na granicy odczytu (obiekt wieloliniowy)
        self._read_buffer = ""
        # Inode ostatnio czytanego pliku â€“ przy rotacji (nowy plik pod tÄ… samÄ… Å›cieÅ¼kÄ…) resetujemy pozycjÄ™
        self._last_inode: Optional[int] = None
        # Liczba kolejnych polli bez nowych wpisÃ³w przy niepustym buforze â€“ wymusza resync przy â€zatkaniuâ€
        self._polls_no_progress: int = 0

    @property
    def entries(self) -> list[dict]:
        """Ostatnie wpisy (najstarsze first)."""
        return list(self._entries)

    def _parse_single(self, obj: dict) -> dict:
        """Kategoryzuje pojedynczy wpis."""
        if not isinstance(obj, dict):
            raise ValueError(f"Oczekiwano dict, otrzymano {type(obj).__name__}")
        try:
            category = categorize(obj)
            obj["_category"] = {
                "name": category.name,
                "display_name": category.display_name,
                "severity": category.severity,
                "icon": category.icon,
                "color": category.color,
                "tags": list(category.tags) if category.tags else [],
            }
        except Exception as e:
            logger.exception("BÅ‚Ä…d kategoryzacji: %s", e)
            obj["_category"] = {
                "name": "unknown",
                "display_name": "Inne",
                "severity": "info",
                "icon": "ğŸ“„",
                "color": "#9ca3af",
                "tags": ["uncategorized", "parse_error"],
            }
        return obj

    def _read_json_lines(self, text: str) -> tuple[list[dict], str]:
        """
        Parsuje tekst â€“ JSON moÅ¼e zajmowaÄ‡ wiele linii (full_log z \\n).
        UÅ¼ywa raw_decode; zwraca (lista wpisÃ³w, nieparsowany ogon bufora).
        """
        entries = []
        buffer = text.lstrip()
        decoder = json.JSONDecoder()
        while buffer:
            try:
                obj, idx = decoder.raw_decode(buffer)
            except json.JSONDecodeError:
                break
            if isinstance(obj, dict):
                try:
                    entries.append(self._parse_single(obj))
                except Exception as ex:
                    logger.warning("PominiÄ™to wpis (bÅ‚Ä…d przetwarzania): %s", ex)
            else:
                logger.debug("PominiÄ™to wpis (nie jest obiektem): %s", type(obj).__name__)
            buffer = buffer[idx:].lstrip()
        return entries, buffer

    def _effective_archives_path(self) -> Optional[Path]:
        """ÅšcieÅ¼ka do aktualnego pliku: resolve() gdy plik istnieje (obsÅ‚uga symlinkÃ³w i rotacji)."""
        if not self.archives_path.exists():
            return None
        try:
            return self.archives_path.resolve()
        except OSError:
            return self.archives_path

    def _read_new_lines(self) -> list[dict]:
        """Odczytuje nowe linie od ostatniej pozycji w pliku. Buforuje niepeÅ‚ny JSON.
        Wykrywa rotacjÄ™/truncate: gdy plik siÄ™ zmieniÅ‚ (inode) lub pozycja > rozmiar, resetuje Å›ledzenie.
        Zawsze otwiera plik po Å›cieÅ¼ce (resolve przy symlinku), Å¼eby po rotacji Wazuh czytaÄ‡ aktualny plik.
        """
        path = self._effective_archives_path()
        if path is None:
            return []

        new_entries = []
        try:
            try:
                stat_info = path.stat()
                current_inode = stat_info.st_ino
                current_size = stat_info.st_size
            except OSError:
                current_inode = None
                current_size = 0

            # Rotacja: pod tÄ… samÄ… Å›cieÅ¼kÄ… jest inny plik (nowy inode)
            if self._last_inode is not None and current_inode is not None and current_inode != self._last_inode:
                logger.warning(
                    "Wykryto rotacjÄ™ pliku archiwum (inode %s -> %s), reset Å›ledzenia",
                    self._last_inode, current_inode,
                )
                self._file_position = 0
                self._read_buffer = ""
                self._polls_no_progress = 0
            # Truncate lub nowy pusty plik: zapisana pozycja jest za koÅ„cem pliku
            elif self._file_position > current_size:
                logger.warning(
                    "Pozycja pliku (%s) za koÅ„cem (rozmiar %s), reset Å›ledzenia",
                    self._file_position, current_size,
                )
                self._file_position = 0
                self._read_buffer = ""
                self._polls_no_progress = 0

            with open(path, "r", encoding="utf-8", errors="replace") as f:
                if self._file_position > 0:
                    try:
                        f.seek(self._file_position)
                    except OSError:
                        self._file_position = 0

                new_content = f.read()
                self._file_position = f.tell()
                self._last_inode = current_inode
                combined = self._read_buffer + new_content
                new_entries, self._read_buffer = self._read_json_lines(combined)

            # NiepeÅ‚ny JSON nie moÅ¼e rosnÄ…Ä‡ w nieskoÅ„czonoÅ›Ä‡ (np. uciÄ™ty wpis na koÅ„cu pliku)
            if len(self._read_buffer) > self.max_read_buffer_bytes:
                logger.warning(
                    "Bufor niepeÅ‚nego JSON przekroczyÅ‚ %s B (%s B), reset bufora i resync do koÅ„ca pliku",
                    self.max_read_buffer_bytes, len(self._read_buffer),
                )
                self._read_buffer = ""
                self._file_position = current_size  # resync: nastÄ™pny odczyt tylko nowe dane, bez ponownego wczytywania ogona
                self._polls_no_progress = 0
            else:
                # Brak postÄ™pu: od wielu polli nie ma nowych wpisÃ³w, a bufor nie jest pusty â€“ wymuszamy resync
                if new_entries:
                    self._polls_no_progress = 0
                elif self._read_buffer:
                    self._polls_no_progress += 1
                    if self._polls_no_progress >= self.STALE_BUFFER_POLLS:
                        logger.warning(
                            "Brak nowych wpisÃ³w od %s polli (bufor %s B), resync do koÅ„ca pliku",
                            self._polls_no_progress, len(self._read_buffer),
                        )
                        self._read_buffer = ""
                        self._file_position = current_size
                        self._polls_no_progress = 0
                else:
                    self._polls_no_progress = 0
        except (IOError, OSError) as e:
            logger.error("BÅ‚Ä…d odczytu pliku: %s", e)
            if self.on_read_error:
                try:
                    self.on_read_error()
                except Exception:
                    pass

        return new_entries

    def _read_tail_lines(self, n: int) -> tuple[list[dict], str]:
        """Czyta tylko ostatnie n linii z pliku (szybszy start przy duÅ¼ych plikach)."""
        path = self._effective_archives_path()
        if path is None or n <= 0:
            return [], ""
        try:
            with open(path, "rb") as f:
                f.seek(0, 2)
                size = f.tell()
                # Szacunkowo ~1KB na liniÄ™ (logi bywajÄ… dÅ‚ugie); czytaj od pozycji na n linii
                chunk_size = min(size, max(2 * 1024 * 1024, n * 1000))
                start = max(0, size - chunk_size)
                f.seek(start)
                if start > 0:
                    f.readline()  # prawdopodobnie uciÄ™ta linia â€“ pomiÅ„
                lines = []
                for line in f:
                    lines.append(line.decode("utf-8", errors="replace"))
                lines = lines[-n:]  # ostatnie n linii (moÅ¼e byÄ‡ fragment wieloliniowego JSON)
        except (IOError, OSError) as e:
            logger.error("BÅ‚Ä…d odczytu tail: %s", e)
            if self.on_read_error:
                try:
                    self.on_read_error()
                except Exception:
                    pass
            return [], ""
        text = "".join(lines)
        entries, leftover = self._read_json_lines(text)
        return entries, leftover

    def load_initial(self) -> int:
        """
        Åaduje ostatnie max_logs wpisÃ³w (szybki start bez czytania caÅ‚ego pliku).
        Ustawia _file_position na koniec pliku; niepeÅ‚ny JSON trafia do _read_buffer.
        """
        entries, self._read_buffer = self._read_tail_lines(self.max_logs)
        for e in entries:
            self._entries.append(e)
            if self.on_new_entry:
                try:
                    self.on_new_entry(e)
                except Exception as ex:
                    logger.exception("BÅ‚Ä…d w on_new_entry (load_initial): %s", ex)
        path = self._effective_archives_path()
        if path is not None:
            try:
                with open(path, "r", encoding="utf-8", errors="replace") as f:
                    f.seek(0, 2)
                    self._file_position = f.tell()
                try:
                    self._last_inode = path.stat().st_ino
                except OSError:
                    self._last_inode = None
            except (IOError, OSError):
                self._file_position = 0
                self._last_inode = None
        else:
            self._file_position = 0
            self._last_inode = None
        return len(entries)

    def poll(self) -> list[dict]:
        """
        Sprawdza nowe wpisy i zwraca listÄ™ nowo dodanych.
        """
        new_entries = self._read_new_lines()
        for e in new_entries:
            self._entries.append(e)
            if self.on_new_entry:
                try:
                    self.on_new_entry(e)
                except Exception as ex:
                    logger.exception("BÅ‚Ä…d w on_new_entry: %s", ex)
        return new_entries

    def run_tail(self):
        """PÄ™tla tail - niekoÅ„czÄ…ce siÄ™ Å›ledzenie pliku."""
        self._running = True
        self.load_initial()
        logger.info("ZaÅ‚adowano %d wpisÃ³w, Å›ledzÄ™ plik...", len(self._entries))
        while self._running:
            self.poll()
            time.sleep(self.poll_interval)

    def stop(self):
        """Zatrzymuje pÄ™tlÄ™ tail."""
        self._running = False
